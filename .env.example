# Java Chat environment variables
#
# Copy to .env:
#   cp .env.example .env

# Environment Configuration
# Set to 'prod' for production (hides API key details in logs)
# Set to 'dev' for development (shows last 4 chars of API keys in logs)
SPRING_PROFILE=dev

# Server Configuration
PORT=8085

# LLM providers (set one or both)
#
# GitHub Models (free tier available)
GITHUB_TOKEN=
# Optional overrides (used by Makefile + streaming SDK)
GITHUB_MODELS_BASE_URL=https://models.github.ai/inference
GITHUB_MODELS_CHAT_MODEL=gpt-5
#
# OpenAI (optional: can be used as primary or fallback)
OPENAI_API_KEY=
# For OpenAI: https://api.openai.com
OPENAI_BASE_URL=https://api.openai.com
OPENAI_MODEL=gpt-5.2
# OPENAI_REASONING_EFFORT=high

# Local Embedding Server (if you have one running)
APP_LOCAL_EMBEDDING_ENABLED=false
LOCAL_EMBEDDING_SERVER_URL=http://127.0.0.1:8088
APP_LOCAL_EMBEDDING_MODEL=text-embedding-qwen3-embedding-8b
APP_LOCAL_EMBEDDING_DIMENSIONS=4096
APP_LOCAL_EMBEDDING_USE_HASH_WHEN_DISABLED=true

# Qdrant Vector Database (local docker-compose defaults)
QDRANT_HOST=localhost
QDRANT_SSL=false
QDRANT_PORT=8086       # gRPC (app)
QDRANT_REST_PORT=8087  # REST (scripts/monitors)
QDRANT_COLLECTION=java-chat
QDRANT_API_KEY=

# RAG Configuration
RAG_CHUNK_MAX_TOKENS=900
RAG_CHUNK_OVERLAP_TOKENS=150
RAG_TOP_K=12
RAG_RETURN_K=6
RAG_CITATIONS_K=3
