spring.application.name=java-chat

# Spring Profile (dev, prod)
spring.profiles.active=${SPRING_PROFILE:prod}

# HTTP server (restricted to 8085-8090 by PortInitializer)
server.port=${PORT:8085}
server.forward-headers-strategy=framework

# Canonical public base URL used for SEO responses (sitemap.xml, robots.txt, OpenGraph/canonical tags)
app.public-base-url=${PUBLIC_BASE_URL:https://javachat.ai}

# Memory-sensitive defaults for 512MB container budgets
# Note: lazy-initialization=true defers bean creation to first use, reducing startup memory but moving errors to runtime
spring.main.lazy-initialization=${SPRING_MAIN_LAZY_INITIALIZATION:true}
spring.http.codecs.max-in-memory-size=${SPRING_HTTP_CODECS_MAX_IN_MEMORY_SIZE:1MB}

# Spring AI - GitHub Models Configuration (Primary)
# CRITICAL: GitHub Models endpoint is https://models.github.ai/inference
# DO NOT USE: models.inference.ai.azure.com (this is a hallucinated URL)
# DO NOT USE: Any azure.com domain (we don't have Azure instances)
# CORRECT ENDPOINT: https://models.github.ai/inference (as per GitHub Models docs)
# Reference: https://docs.github.com/en/github-models/prototyping-with-ai-models
spring.ai.openai.base-url=${OPENAI_BASE_URL:https://models.github.ai/inference}
spring.ai.openai.chat.options.model=${OPENAI_MODEL:gpt-5.2}
spring.ai.openai.chat.options.temperature=${OPENAI_TEMPERATURE:0.7}
# Use GitHub token for GitHub Models, or OpenAI API key for fallback
# To avoid rate limits, set OPENAI_API_KEY in addition to GITHUB_TOKEN
spring.ai.openai.api-key=${GITHUB_TOKEN:${OPENAI_API_KEY:}}
spring.ai.openai.chat.api-key=${GITHUB_TOKEN:${OPENAI_API_KEY:}}

# OpenAI Java SDK streaming configuration (OpenAIStreamingService)
# Base URLs are used by the SDK directly (not Spring AI).
# Defaults are set via @Value annotations in OpenAIStreamingService.java;
# override via environment variables GITHUB_MODELS_BASE_URL or OPENAI_BASE_URL.
# Optional: set OPENAI_REASONING_EFFORT for GPT-5.2 family (e.g., high for gpt-5.2-pro).
# Streaming timeouts: OPENAI_STREAMING_REQUEST_TIMEOUT_SECONDS (default: 600),
# OPENAI_STREAMING_READ_TIMEOUT_SECONDS (default: 75). All set via env vars.
# Provider ordering: LLM_PRIMARY_PROVIDER=github_models (default) or openai.
# If both providers are configured, the secondary provider is used as explicit fallback.

# Rate limiting and retry configuration
# More conservative defaults to handle GitHub Models API rate limits
spring.ai.retry.max-attempts=${AI_RETRY_MAX_ATTEMPTS:5}
spring.ai.retry.backoff.initial-interval=${AI_RETRY_INITIAL_INTERVAL:2000}
spring.ai.retry.backoff.max-interval=${AI_RETRY_MAX_INTERVAL:30000}
spring.ai.retry.backoff.multiplier=${AI_RETRY_MULTIPLIER:2}

# HTTP client timeout configuration for GitHub Models API
# GitHub Models can be slower than OpenAI, so increase timeouts
# Note: Spring AI doesn't expose timeout properties directly
# Timeouts are configured via WebClient/RestClient beans in AiConfig

# Embeddings model (GitHub Models compatible)
# NOTE: GitHub Models doesn't support embeddings API yet
# Configure an embedding provider explicitly; no runtime fallback is allowed
spring.ai.openai.embedding.options.model=${GITHUB_MODELS_EMBED_MODEL:text-embedding-3-small}
spring.ai.openai.embedding.base-url=${OPENAI_EMBEDDING_BASE_URL:https://api.openai.com}
spring.ai.openai.embedding.api-key=${OPENAI_API_KEY:}
# Toggle for local embeddings (when true, uses LocalEmbeddingModel)
# Set to true to use your local embedding server
app.local-embedding.enabled=${APP_LOCAL_EMBEDDING_ENABLED:false}
# Local embedding server configuration - set default to allowed range (8085-8090)
app.local-embedding.server-url=${LOCAL_EMBEDDING_SERVER_URL:http://127.0.0.1:8088}
# Support both APP_LOCAL_EMBEDDING_MODEL and LOCAL_EMBEDDING_MODEL_NAME env vars
app.local-embedding.model=${APP_LOCAL_EMBEDDING_MODEL:${LOCAL_EMBEDDING_MODEL_NAME:text-embedding-qwen3-embedding-8b}}
app.local-embedding.dimensions=${APP_LOCAL_EMBEDDING_DIMENSIONS:4096}
app.local-embedding.batch-size=${APP_LOCAL_EMBEDDING_BATCH_SIZE:32}
# Embedding dimensions (must match the actual embedding model output size)
# Default aligns with the Qwen3 embedding model used for ingestion/retrieval.
app.embeddings.dimensions=${APP_EMBEDDINGS_DIMENSIONS:4096}

# Remote embedding provider (OpenAI-compatible, e.g., Novita)
# Defaults align with Qwen3 embeddings to keep ingestion and retrieval consistent.
app.remote-embedding.server-url=${REMOTE_EMBEDDING_SERVER_URL:https://api.novita.ai/openai/v1/embeddings}
app.remote-embedding.model=${REMOTE_EMBEDDING_MODEL_NAME:qwen/qwen3-embedding-8b}
app.remote-embedding.api-key=${REMOTE_EMBEDDING_API_KEY:}
app.remote-embedding.dimensions=${REMOTE_EMBEDDING_DIMENSIONS:4096}

# Qdrant configuration (defaults to local Docker)
spring.ai.vectorstore.qdrant.host=${QDRANT_HOST:localhost}
spring.ai.vectorstore.qdrant.port=${QDRANT_PORT:6334}
spring.ai.vectorstore.qdrant.api-key=${QDRANT_API_KEY:}
spring.ai.vectorstore.qdrant.use-tls=${QDRANT_SSL:false}
# Hybrid collection config (replaces single-collection Spring AI VectorStore)
app.qdrant.collections.books=${QDRANT_COLLECTION_BOOKS:java-chat-books}
app.qdrant.collections.docs=${QDRANT_COLLECTION_DOCS:java-docs}
app.qdrant.collections.articles=${QDRANT_COLLECTION_ARTICLES:java-articles}
app.qdrant.collections.pdfs=${QDRANT_COLLECTION_PDFS:java-pdfs}
app.qdrant.dense-vector-name=${QDRANT_DENSE_VECTOR_NAME:dense}
app.qdrant.sparse-vector-name=${QDRANT_SPARSE_VECTOR_NAME:bm25}
app.qdrant.prefetch-limit=${HYBRID_PREFETCH_LIMIT:20}
app.qdrant.rrf-k=${HYBRID_RRF_K:60}
app.qdrant.fail-on-partial-search-error=${HYBRID_FAIL_ON_PARTIAL_SEARCH_ERROR:true}
app.qdrant.query-timeout=${HYBRID_QUERY_TIMEOUT:5s}

# Retrieval defaults (match AppProperties.Rag)
app.rag.chunk-max-tokens=${RAG_CHUNK_MAX_TOKENS:900}
app.rag.chunk-overlap-tokens=${RAG_CHUNK_OVERLAP_TOKENS:150}
app.rag.search-top-k=${RAG_TOP_K:12}
app.rag.search-return-k=${RAG_RETURN_K:6}
app.rag.search-citations=${RAG_CITATIONS_K:3}
app.rag.search-mmr-lambda=${RAG_MMR_LAMBDA:0.5}
app.rag.reranker-timeout=${RAG_RERANKER_TIMEOUT:12s}

# LLM defaults used by openai-java streaming service
app.llm.temperature=${APP_LLM_TEMPERATURE:0.7}

# Java 24 docs crawl root (immutable snapshot target)
app.docs.root-url=${DOCS_ROOT_URL:https://docs.oracle.com/en/java/javase/25/}
app.docs.jdk-version=${DOCS_JDK_VERSION:25}
app.docs.snapshot-dir=${DOCS_SNAPSHOT_DIR:data/snapshots}
app.docs.parsed-dir=${DOCS_PARSED_DIR:data/parsed}
app.docs.index-dir=${DOCS_INDEX_DIR:data/index}

# Ensure Qdrant payload indexes on boot so metadata filtering works reliably.
app.qdrant.ensure-payload-indexes=${APP_QDRANT_ENSURE_PAYLOAD_INDEXES:true}

# Static Resources Configuration
# Enable static resource handling with proper cache control
spring.web.resources.static-locations=classpath:/static/,classpath:/public/
spring.web.resources.cache.period=3600
spring.web.resources.chain.strategy.content.enabled=true
spring.web.resources.chain.strategy.content.paths=/**

# CORS Configuration (production defaults)
app.cors.allowed-origins=http://localhost:8085,http://127.0.0.1:8085,https://dev.javachat.ai,https://javachat.ai
app.cors.allowed-methods=GET,POST,PUT,DELETE,OPTIONS
app.cors.allowed-headers=*
app.cors.allow-credentials=true
app.cors.max-age-seconds=3600

# Actuator
management.endpoints.web.exposure.include=health,info,metrics
# Reduce Qdrant client warning verbosity via logging
logging.level.io.qdrant=ERROR
# Suppress PDFBox font mapping warnings (these are harmless)
logging.level.org.apache.pdfbox.pdmodel.font=ERROR

# Spring Security - configuration
# Keep UserDetailsService auto-config disabled to avoid default user + password logging
# but allow SecurityAutoConfiguration so HttpSecurity is available for our filters
# Exclude OpenAI models that require API keys to prevent startup failures
spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.security.servlet.UserDetailsServiceAutoConfiguration,org.springframework.ai.model.openai.autoconfigure.OpenAiAudioSpeechAutoConfiguration,org.springframework.ai.model.openai.autoconfigure.OpenAiAudioTranscriptionAutoConfiguration,org.springframework.ai.model.openai.autoconfigure.OpenAiImageAutoConfiguration,org.springframework.ai.model.openai.autoconfigure.OpenAiModerationAutoConfiguration

# Disable unused Spring AI chat/embedding auto-config by default (we use openai-java for chat)
spring.ai.model.chat=${SPRING_AI_MODEL_CHAT:none}
spring.ai.model.embedding=${SPRING_AI_MODEL_EMBEDDING:none}
